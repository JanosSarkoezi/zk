# Feedforward-Neural Networks

## Frage

Lass uns die Schritte der Backpropagation in einem FFNN matchematisch
beschreiben. Zuerst mit Formeln und dann mit einem Pseudocode. Danke dir.

## Antwort

Backpropagation ist ein Algorithmus, der in k√ºnstlichen neuronalen Netzen
verwendet wird, um die **Gewichte (weights)** und **Bias-Werte (biases)** zu
optimieren. Das Ziel ist, den Fehler des Netzes zu minimieren, indem die
Gradienten der Verlustfunktion bez√ºglich der Gewichte berechnet und diese dann
aktualisiert werden. Der Algorithmus besteht aus zwei Hauptschritten: einem
Vorw√§rtsdurchlauf (Forward Pass) und einem R√ºckw√§rtsdurchlauf (Backward Pass).

-----

## 1\. Vorw√§rtsdurchlauf (Forward Pass)

Im Vorw√§rtsdurchlauf wird die Eingabe durch das neuronale Netz geleitet, um
eine Ausgabe zu erzeugen. F√ºr jede Schicht $l$ und jeden Neuron $j$ wird die
Aktivierung wie folgt berechnet:

1.  **Nettosummierung (Net Input)**:
    $$z_j^{(l)} = \sum_{i} w_{ji}^{(l)} a_i^{(l-1)} + b_j^{(l)}$$
    Hierbei ist $a_i^{(l-1)}$ die Aktivierung des Neurons $i$ in der
    vorhergehenden Schicht $l-1$. $w_{ji}^{(l)}$ sind die Gewichte von Neuron $i$
    in Schicht $l-1$ zu Neuron $j$ in Schicht $l$, und $b_j^{(l)}$ ist der
    Bias-Wert von Neuron $j$ in Schicht $l$.

2.  **Aktivierungsfunktion (Activation Function)**:
    $$a_j^{(l)} = f(z_j^{(l)})$$
    $f$ ist die Aktivierungsfunktion (z.B. Sigmoid, ReLU, Tanh), die auf die
    Nettosummierung angewendet wird, um die endg√ºltige Aktivierung zu erhalten.

-----

## 2\. R√ºckw√§rtsdurchlauf (Backward Pass)

Nach dem Vorw√§rtsdurchlauf wird die Ausgabe des Netzes mit der gew√ºnschten
Ausgabe (dem Label) verglichen, um den Fehler zu berechnen. F√ºr diesen Schritt
ben√∂tigen wir eine **Verlustfunktion (loss function)**, z.B. die quadratische
Fehlerfunktion (Mean Squared Error, MSE):

$$E = \frac{1}{2} \sum_{k} (y_k - a_k^{(L)})^2$$

Hierbei ist $y_k$ der Sollwert und $a_k^{(L)}$ die tats√§chliche Ausgabe des Netzes (in der letzten Schicht $L$).

Der Kern der Backpropagation ist nun die Berechnung der Gradienten des Fehlers bez√ºglich der Gewichte und Bias-Werte, um diese zu aktualisieren.

1.  **Fehler (Delta) der Ausgabeschicht:**
    F√ºr die letzte Schicht $L$ wird das Fehlerdelta $\delta_k^{(L)}$ wie folgt berechnet:
    $$\delta_k^{(L)} = (a_k^{(L)} - y_k) \cdot f'(z_k^{(L)})$$
    $f'$ ist die Ableitung der Aktivierungsfunktion. F√ºr MSE ist der Term
    $(a_k^{(L)} - y_k)$ die Ableitung des Fehlers nach der Aktivierung.

2.  **Fehler (Delta) der verborgenen Schichten:**
    F√ºr jede verborgene Schicht $l$ (von $L-1$ bis 1) wird das Fehlerdelta $\delta_j^{(l)}$ rekursiv von der nachfolgenden Schicht $l+1$ berechnet:
    $$\delta_j^{(l)} = \left(\sum_{k} w_{kj}^{(l+1)} \delta_k^{(l+1)}\right) \cdot f'(z_j^{(l)})$$
    Die Summe √ºber $k$ repr√§sentiert die gewichtete Summe der Fehlerdeltas der nachfolgenden Schicht.

3.  **Berechnung der Gradienten:**
    Mit den Deltas k√∂nnen nun die Gradienten f√ºr die Gewichte und Bias-Werte berechnet werden.

      * **Gradient f√ºr Gewichte:**
        $$\frac{\partial E}{\partial w_{ji}^{(l)}} = a_i^{(l-1)} \cdot \delta_j^{(l)}$$
      * **Gradient f√ºr Bias:**
        $$\frac{\partial E}{\partial b_{j}^{(l)}} = \delta_j^{(l)}$$

4.  **Aktualisierung der Gewichte und Bias-Werte:**
    Die Gewichte und Bias-Werte werden mit einem Lernrateparameter $\eta$ (eta) aktualisiert:

      * **Gewichtsaktualisierung:**
        $$w_{ji}^{(l)} = w_{ji}^{(l)} - \eta \frac{\partial E}{\partial w_{ji}^{(l)}}$$
      * **Bias-Aktualisierung:**
        $$b_{j}^{(l)} = b_{j}^{(l)} - \eta \frac{\partial E}{\partial b_{j}^{(l)}}$$
        Dieser Prozess wird f√ºr eine bestimmte Anzahl von Epochen oder bis zur Konvergenz wiederholt.

-----

## Pseudocode f√ºr Backpropagation

```
Funktion Backpropagation(Netzwerk, Eingabe, Sollwert, Lernrate):
    // 1. Vorw√§rtsdurchlauf
    F√ºr jede Schicht l von 1 bis L:
        Berechne die gewichtete Summe z f√ºr jedes Neuron:
            z_j^(l) = Summe(w_ji^(l) * a_i^(l-1)) + b_j^(l)
        Berechne die Aktivierung a:
            a_j^(l) = Aktivierungsfunktion(z_j^(l))

    // 2. Fehlerberechnung
    Berechne den Fehler E f√ºr die Ausgabeschicht L:
        E = 0.5 * Summe((Sollwert_k - Ausgabe_k)^2)
        
    // 3. R√ºckw√§rtsdurchlauf
    // Berechne Delta f√ºr die Ausgabeschicht
    F√ºr jedes Neuron k in der Ausgabeschicht L:
        delta_k^(L) = (a_k^(L) - Sollwert_k) * Ableitung_Aktivierungsfunktion(z_k^(L))

    // Berechne Delta f√ºr die verborgenen Schichten
    F√ºr jede Schicht l von L-1 bis 1 (r√ºckw√§rts):
        F√ºr jedes Neuron j in der Schicht l:
            delta_j^(l) = (Summe(w_kj^(l+1) * delta_k^(l+1))) * Ableitung_Aktivierungsfunktion(z_j^(l))

    // 4. Gradienten berechnen und Gewichte/Bias aktualisieren
    F√ºr jede Schicht l von 1 bis L:
        F√ºr jedes Neuron j in der Schicht l:
            F√ºr jedes Neuron i in der vorherigen Schicht l-1:
                Gradient_w = a_i^(l-1) * delta_j^(l)
                w_ji^(l) = w_ji^(l) - Lernrate * Gradient_w
            
            Gradient_b = delta_j^(l)
            b_j^(l) = b_j^(l) - Lernrate * Gradient_b
            
    Gib das aktualisierte Netzwerk zur√ºck
```

## Frage

Kannst du bitte die kompakte mathematische Form eines Trainingforganges des
FFNN schildern? Danke dir.

## Antowrt

Absolut! Hier ist die kompakte mathematische Formulierung eines vollst√§ndigen
Trainingsdurchgangs (Epoche) eines Feedforward-Neural Networks (FFNN) mit
Backpropagation und SGD.

## Gegeben:
- **Eingabevektor**: $\mathbf{x} \in \mathbb{R}^{n}$
- **Zielvektor**: $\mathbf{y} \in \mathbb{R}^{m}$
- **Gewichtsmatrizen**: $W^{(l)}$ f√ºr Schicht $l$
- **Bias-Vektoren**: $\mathbf{b}^{(l)}$ f√ºr Schicht $l$
- **Aktivierungsfunktion**: $\sigma(\cdot)$ (z.B. Sigmoid, ReLU)
- **Loss-Funktion**: $L(\mathbf{\hat{y}}, \mathbf{y})$ (z.B. MSE, Cross-Entropy)
- **Lernrate**: $\eta$

---

## 1. Forward Pass (Vorw√§rtsdurchlauf)

**F√ºr jede Schicht $l = 1, ..., L$:**
$$
\begin{align*}
\mathbf{z}^{(l)} &= W^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)} \quad &\text{(Netz-Eingabe)} \\
\mathbf{a}^{(l)} &= \sigma(\mathbf{z}^{(l)}) \quad &\text{(Aktivierung)} \\
\end{align*}
$$
mit $\mathbf{a}^{(0)} = \mathbf{x}$ (Eingabe) und $\mathbf{a}^{(L)} = \mathbf{\hat{y}}$ (Ausgabe)

**Loss-Berechnung:**
$$
J = L(\mathbf{\hat{y}}, \mathbf{y})
$$

---

## 2. Backward Pass (R√ºckw√§rtsdurchlauf - Backpropagation)

**Outputschicht ($l = L$):**
$$
\begin{align*}
\mathbf{\delta}^{(L)} &= \nabla_{\mathbf{\hat{y}}} L \odot \sigma'(\mathbf{z}^{(L)}) \quad &\text{(Fehlerterm)} \\
\frac{\partial J}{\partial W^{(L)}} &= \mathbf{\delta}^{(L)} (\mathbf{a}^{(L-1)})^T \\
\frac{\partial J}{\partial \mathbf{b}^{(L)}} &= \mathbf{\delta}^{(L)}
\end{align*}
$$

**F√ºr versteckte Schichten ($l = L-1, ..., 1$):**
$$
\begin{align*}
\mathbf{\delta}^{(l)} &= \left( (W^{(l+1)})^T \mathbf{\delta}^{(l+1)} \right) \odot \sigma'(\mathbf{z}^{(l)}) \\
\frac{\partial J}{\partial W^{(l)}} &= \mathbf{\delta}^{(l)} (\mathbf{a}^{(l-1)})^T \\
\frac{\partial J}{\partial \mathbf{b}^{(l)}} &= \mathbf{\delta}^{(l)}
\end{align*}
$$

---

## 3. Parameterupdate (Gradient Descent)

**F√ºr alle Schichten $l = 1, ..., L$:**
$$
\begin{align*}
W^{(l)} &\leftarrow W^{(l)} - \eta \frac{\partial J}{\partial W^{(l)}} \\
\mathbf{b}^{(l)} &\leftarrow \mathbf{b}^{(l)} - \eta \frac{\partial J}{\partial \mathbf{b}^{(l)}}
\end{align*}
$$

---

## üîÑ Kompakter Algorithmus (Pseudocode)

```
F√ºr jede Trainingsepoche:
    F√ºr jeden Datenpunkt (x, y) im Batch:
        # Forward Pass
        a‚Å∞ = x
        F√ºr l = 1 bis L:
            zÀ° = WÀ°¬∑aÀ°‚Åª¬π + bÀ°
            aÀ° = œÉ(zÀ°)
        
        # Loss Berechnung
        J = L(a·¥∏, y)
        
        # Backward Pass
        Œ¥·¥∏ = ‚àá_a·¥∏ L ‚äô œÉ'(z·¥∏)
        F√ºr l = L-1 bis 1:
            Œ¥À° = (WÀ°‚Å∫¬π)·µÄ¬∑Œ¥À°‚Å∫¬π ‚äô œÉ'(zÀ°)
        
        # Gradienten akkumulieren
        F√ºr l = 1 bis L:
            ‚àáWÀ° += Œ¥À°¬∑(aÀ°‚Åª¬π)·µÄ
            ‚àábÀ° += Œ¥À°
    
    # Parameterupdate (nach Batch)
    F√ºr l = 1 bis L:
        WÀ° -= Œ∑ ¬∑ ‚àáWÀ°
        bÀ° -= Œ∑ ¬∑ ‚àábÀ°
```

---

## üéØ Wichtige Konzepte in kompakter Form:

- **$\odot$**: Elementweises Produkt (Hadamard-Produkt)
- **$\sigma'(\cdot)$**: Ableitung der Aktivierungsfunktion
- **$\delta^{(l)}$**: Fehlerterm der Schicht $l$ ("Wie sehr ist jede Neuron schuld?")
- **Chain Rule**: $\frac{\partial J}{\partial W} = \frac{\partial J}{\partial a} \frac{\partial a}{\partial z} \frac{\partial z}{\partial W}$

Dieser Prozess wird iterativ wiederholt bis Konvergenz erreicht ist!
